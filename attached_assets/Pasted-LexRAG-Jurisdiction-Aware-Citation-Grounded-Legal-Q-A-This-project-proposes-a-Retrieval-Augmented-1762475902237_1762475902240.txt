LexRAG: Jurisdiction-Aware, Citation-Grounded Legal Q&A
This project proposes a Retrieval-Augmented Generation (RAG) system tailored for legal research. The goal is to help lawyers, students, and analysts efficiently locate and interpret relevant cases, statutes, and regulations while ensuring responses are accurate, jurisdiction-specific, and properly cited.
The application is designed to operate across multiple jurisdictions by tagging each document with metadata such as court, jurisdiction, citation, and date. When a user poses a question, the retriever can filter and rank sources according to jurisdictional scope—for example, distinguishing between federal and state-level precedents or selecting regulatory summaries rather than litigation records. This structure allows the same system to support diverse research modes, including comparative case law analysis, recent regulatory updates, and litigation-related precedent discovery.
Data ingestion will leverage existing open legal data sources and APIs rather than manual collection. The system can integrate with platforms such as CourtListener (for case law opinions), GovInfo and eCFR (for federal regulations), and Regulations.gov (for rulemaking comments and updates). Each source will be normalized into a unified schema and stored locally in JSONL format for reproducibility. This approach allows a limited but representative corpus to be built automatically, making the project practical for an academic timeframe.
Although the core pipeline will function as a standard RAG system—retrieving passages and generating grounded, citation-rich answers—it is structured so that it could later evolve toward a more agentic framework. In such an extension, the model could plan multi-step reasoning: first identifying the correct jurisdiction, then retrieving supporting regulations, verifying citation accuracy, and finally synthesizing a cohesive answer. Implementing full multi-step tool orchestration is not necessary for this project’s scope, but the design will accommodate it conceptually, with hooks for later expansion.
Evaluation will combine retrieval metrics and model-based assessment. Retrieval quality will be measured using Recall@K and Mean Reciprocal Rank, while factual grounding, citation correctness, and jurisdictional alignment will be evaluated using an “AI-as-judge” framework. A secondary language model such as Claude or GPT-4 will review each answer in the context of its retrieved sources and assign structured scores for factual accuracy, citation validity, and relevance. This allows for scalable and consistent evaluation without requiring manual annotation, while still aligning with current research practices in LLM evaluation.
In summary, LexRAG will demonstrate how retrieval-augmented generation can be adapted to the legal domain through structured preprocessing, automated data ingestion, jurisdiction-aware retrieval, and AI-based evaluation. The project’s design emphasizes both practical feasibility and extensibility toward more advanced, agentic legal reasoning systems.

